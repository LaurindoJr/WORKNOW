# ğŸ“š Biblioteca na Nuvem â€“ KCL
### AplicaÃ§Ã£o Web + Processamento AssÃ­ncrono + Arquitetura ElÃ¡stica
*(EC2 | RDS | S3 | DynamoDB | SQS | ALB | ASG | CloudWatch)*

---

# ğŸš€ Parte 1 â€” Arquitetura da AplicaÃ§Ã£o

A aplicaÃ§Ã£o "Biblioteca na Nuvem â€“ KCL" utiliza cinco serviÃ§os principais da AWS para fornecer um ambiente escalÃ¡vel, desacoplado e resiliente.

---

## **1ï¸âƒ£ Interface Web (Flask em EC2)**

A aplicaÃ§Ã£o web foi desenvolvida em **Flask** e Ã© executada em uma instÃ¢ncia **EC2**.

**FunÃ§Ãµes principais:**
- Interface web para cadastrar, editar, excluir e listar livros.
- Registro de aluguÃ©is (rentals).
- Upload de imagens dos livros para o S3.

**ğŸ‘‰ ServiÃ§o AWS utilizado:** **EC2**  
**ğŸ‘‰ FunÃ§Ã£o:** Hospedar e executar o backend e o frontend.

---

## **2ï¸âƒ£ Banco de Dados Relacional â€” Amazon RDS (PostgreSQL)**

Todas as informaÃ§Ãµes estruturadas da aplicaÃ§Ã£o sÃ£o persistidas em um banco relacional:

- Tabela **books**
- Tabela **rentals**

O Flask realiza operaÃ§Ãµes CRUD diretamente no banco.

**ğŸ‘‰ ServiÃ§o AWS utilizado:** **RDS (PostgreSQL)**  
**ğŸ‘‰ FunÃ§Ã£o:** Armazenamento persistente dos dados dos livros e aluguÃ©is.

---

## **3ï¸âƒ£ Armazenamento de Arquivos â€” Amazon S3**

As imagens enviadas na aplicaÃ§Ã£o sÃ£o armazenadas no bucket S3:

- Upload original em `uploads/`
- Thumbnail gerada automaticamente em `thumb/`

**ğŸ‘‰ ServiÃ§o AWS utilizado:** **S3**  
**ğŸ‘‰ FunÃ§Ã£o:** Armazenamento dos arquivos binÃ¡rios (imagens).

---

## **4ï¸âƒ£ Processamento AssÃ­ncrono â€” Amazon SQS + Worker**

Sempre que uma imagem Ã© enviada, o Flask publica uma mensagem na fila **SQS**:

{"bucket": "biblioteca-kcl", "key": "uploads/dom.jpg"}

Um worker Python (`sqs_worker.py`) lÃª esta mensagem e executa:

1. **Baixa a imagem do S3**
2. **Gera a miniatura (thumbnail)**
3. **Salva no S3** (`thumb/...`)
4. **Atualiza tabela `ProcessingStatus` no DynamoDB**
5. **Cria log** na tabela `kcl-AuditLogs`

ğŸ‘‰ **ServiÃ§o AWS utilizado:** SQS  
ğŸ‘‰ **FunÃ§Ã£o:** Desacoplar o upload da imagem do processamento (pipeline assÃ­ncrono).

---

## **5ï¸âƒ£ Banco NoSQL â€” Amazon DynamoDB**

O DynamoDB Ã© usado para armazenar logs e status de processamento.

### ğŸ“Œ **Tabela 1 â€” `kcl-AuditLogs`**
- `pk`: `APP#CREATE` | `APP#UPDATE` | `APP#DELETE`
- `sk`: UUID
- `data`: JSON com os dados alterados
- `ts`: timestamp ISO-8601

### ğŸ“Œ **Tabela 2 â€” `ProcessingStatus`**
- `pk`: caminho do arquivo
- `status`: `PENDING` | `DONE` | `ERROR`
- `message`: detalhes do processamento

ğŸ‘‰ **ServiÃ§o AWS utilizado:** DynamoDB  
ğŸ‘‰ **FunÃ§Ã£o:** Logs de auditoria + monitoramento do pipeline de imagens.

# Parte 2 - ImplementaÃ§Ã£o de AplicaÃ§Ã£o ElÃ¡stica na AWS - KCL


### Link do vÃ­deo da aplicaÃ§Ã£o sendo executada:

<https://youtu.be/nmVzdnmKXTA>

---
#### Fase 1: PreparaÃ§Ã£o da Imagem (Golden AMI)

O primeiro passo foi criar um "molde" ou "imagem de ouro" (Golden AMI) da aplicaÃ§Ã£o. Isso garante que cada nova instÃ¢ncia provisionada pelo Auto Scaling Group seja idÃªntica e esteja pronta para receber trÃ¡fego.

1.  **Provisionamento da InstÃ¢ncia Base:** Uma instÃ¢ncia EC2 (tipo `t2.micro`) foi lanÃ§ada utilizando uma AMI padrÃ£o (ex: Amazon Linux 2).
2.  **InstalaÃ§Ã£o da AplicaÃ§Ã£o:** A aplicaÃ§Ã£o de biblioteca Python e todas as suas dependÃªncias (ex: `pip install -r requirements.txt`) foram instaladas e configuradas.
3.  **ConfiguraÃ§Ã£o do ServiÃ§o:** Foi configurado um serviÃ§o (ex: via `systemd`) para garantir que a aplicaÃ§Ã£o Python inicie automaticamente junto com o sistema operacional.
4.  **CriaÃ§Ã£o da AMI:** ApÃ³s validar que a aplicaÃ§Ã£o estava funcional na instÃ¢ncia, uma **Amazon Machine Image (AMI)** personalizada foi criada a partir dela. Esta AMI serviu como base para todas as futuras instÃ¢ncias.

---

#### Fase 2: ConfiguraÃ§Ã£o do Balanceador de Carga (ALB)

Para distribuir o trÃ¡fego de forma eficiente e prover um ponto de acesso Ãºnico, um Application Load Balancer foi configurado.

1.  **CriaÃ§Ã£o do Load Balancer:** Um ALB (tipo *Application*) foi criado, configurado para ser *internet-facing* e associado Ã s sub-redes pÃºblicas (em pelo menos duas Zonas de Disponibilidade para alta disponibilidade).
2.  **CriaÃ§Ã£o do Target Group (Grupo de Destino):** Foi criado um Target Group (tipo *Instance*) para o qual o ALB encaminharÃ¡ o trÃ¡fego.
3.  **ConfiguraÃ§Ã£o do Health Check:** O Target Group foi configurado com uma verificaÃ§Ã£o de saÃºde (Health Check) apontando para um endpoint da aplicaÃ§Ã£o (ex: `HTTP /` ou `/health`). O ALB usarÃ¡ isso para saber se uma instÃ¢ncia estÃ¡ saudÃ¡vel antes de enviar trÃ¡fego para ela.
4.  **ConfiguraÃ§Ã£o do Listener:** Um *Listener* foi adicionado ao ALB na porta HTTP 80, com a regra padrÃ£o de encaminhar (forward) o trÃ¡fego para o Target Group criado.

---

#### Fase 3: ConfiguraÃ§Ã£o do Auto Scaling Group (ASG)

O ASG Ã© o cÃ©rebro da elasticidade. Ele foi configurado para gerenciar o ciclo de vida das instÃ¢ncias EC2.

1.  **CriaÃ§Ã£o do Launch Template (Modelo de LanÃ§amento):** Foi criado um *Launch Template* especificando:
    * A **AMI** personalizada (criada na Fase 1).
    * O **Tipo de InstÃ¢ncia** (`t2.micro`, conforme requisito 'a').
    * O **Security Group** (permitindo trÃ¡fego apenas do ALB na porta da aplicaÃ§Ã£o).
2.  **CriaÃ§Ã£o do Auto Scaling Group:** Um ASG foi criado utilizando o Launch Template acima.
3.  **ConfiguraÃ§Ã£o de Rede e AssociaÃ§Ã£o ao ALB:** O ASG foi configurado para lanÃ§ar instÃ¢ncias nas mesmas sub-redes do ALB e, crucialmente, foi associado ao **Target Group** (criado na Fase 2). Isso garante que qualquer instÃ¢ncia nova seja automaticamente registrada no Load Balancer.
4.  **DefiniÃ§Ã£o de Tamanho do Grupo (Requisitos 'a' e 'c'):**
    * **Capacidade Desejada (Desired):** 1
    * **MÃ­nimo (Min):** 1
    * **MÃ¡ximo (Max):** 3

---

#### Fase 4: DefiniÃ§Ã£o das PolÃ­ticas de Elasticidade (CloudWatch)

Finalmente, as regras de negÃ³cio para a elasticidade foram implementadas usando alarmes do CloudWatch e polÃ­ticas de escalonamento.

1.  **Alarme e PolÃ­tica de Scale-Out (Requisito 'c'):**
    * **Alarme (CloudWatch):** Criado o alarme `scale-out-70`.
    * **MÃ©trica:** `CPUUtilization` (MÃ©dia) do ASG.
    * **CondiÃ§Ã£o:** `> 70%`
    * **PerÃ­odo:** `por 1 minuto` (1 perÃ­odo consecutivo de 60 segundos).
    * **PolÃ­tica (ASG):** Criada uma polÃ­tica do tipo *Step Scaling* associada a este alarme.
    * **AÃ§Ã£o:** `Add 1 instance`.

2.  **Alarme e PolÃ­tica de Scale-In (Requisito 'd'):**
    * **Alarme (CloudWatch):** Criado o alarme `scale-in-25`.
    * **MÃ©trica:** `CPUUtilization` (MÃ©dia) do ASG.
    * **CondiÃ§Ã£o:** `< 25%`
    * **PerÃ­odo:** `por 1 minuto` (1 perÃ­odo consecutivo de 60 segundos).
    * **PolÃ­tica (ASG):** Criada uma polÃ­tica do tipo *Step Scaling* associada a este alarme.
    * **AÃ§Ã£o:** `Remove 1 instance`.


### 4. ValidaÃ§Ã£o e Testes

Para validar a arquitetura, foram realizados testes de carga simulados:

1.  **Teste de Scale-Out:** Foi utilizada uma ferramenta de stress de CPU (ex: `stress-ng` ou um script de loop infinito) em uma das instÃ¢ncias para forÃ§ar a mÃ©dia de CPU do grupo a ultrapassar 70%.
    * **Resultado Esperado:** O alarme `scale-out-70` disparou, o ASG iniciou uma nova instÃ¢ncia (atÃ© o mÃ¡ximo de 3). A nova instÃ¢ncia foi registrada no ALB e comeÃ§ou a receber trÃ¡fego, diluindo a carga.
2.  **Teste de Scale-In:** O teste de carga foi interrompido. A utilizaÃ§Ã£o de CPU caiu.
    * **Resultado Esperado:** ApÃ³s a mÃ©dia de CPU do grupo ficar abaixo de 25% por 1 minuto, o alarme `scale-in-25` disparou, e o ASG finalizou uma das instÃ¢ncias (atÃ© o mÃ­nimo de 1).
